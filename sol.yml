---

- name: Create AWS resources
  connection: local
  gather_facts: false
  hosts: localhost
  tasks:
    - block:
        - set_fact:
            aws_region: "eu-west-1"
            destinationCidrBlock: "0.0.0.0/0"
            instance_architecture: "amd64" # amd64 for t3/t3a or arm64 for t4g
        - set_fact:
            vpc_cidr_block: "10.50.0.0/16"
            vpc_name: "testvpc1"
            vpc_acl_name: "testNACL1"
            asg_name: "testASG1"
            asg_lc_name: "testASGlc1"
            vpc_subnets:
              public-a:
                cidr: "10.50.1.0/24"
                az: "{{ aws_region }}a"
              public-b:
                cidr: "10.50.2.0/24"
                az: "{{ aws_region }}b"
              public-c:
                cidr: "10.50.3.0/24"
                az: "{{ aws_region }}c"
              private-a:
                cidr: "10.50.4.0/24"
                az: "{{ aws_region }}a"
              private-b:
                cidr: "10.50.5.0/24"
                az: "{{ aws_region }}b"
              private-c:
                cidr: "10.50.6.0/24"
                az: "{{ aws_region }}c"
            key_name: "{{ lookup('env', 'AWS_KEYPAIR_NAME') }}"
            ami_image:
              owner: 099720109477 # ubuntu/canonical
              name: "ubuntu/images/hvm-ssd/ubuntu-focal-20.04-{{ instance_architecture }}-server-"
              # aws ec2 describe-images --owners 099720109477 --output=text | grep ubuntu/images/hvm-ssd/ubuntu-focal-20.04-
            instance_type: 't3.2xlarge'  # amd64 - match architecture to ami_image
            #instance_type: 't4g.2xlarge' # arm64
            volume_size: '25'
            vpc_security_groups:
              - name: vpc
                description: "Allow internal traffic in the VPC"
                rules:
                  - proto: -1
                    group_name: vpc
                    ports: [-1]
              - name: allow-public-ssh
                description: "Allow public SSH"
                rules:
                  - proto: tcp
                    cidr_ip: "{{ destinationCidrBlock }}"
                    ports: [22]
              - name: allow-public-http
                description: "Allow public web traffic"
                rules:
                  - proto: tcp
                    cidr_ip: "{{ destinationCidrBlock }}"
                    ports: [8000-10000]
                  - proto: udp
                    cidr_ip: "{{ destinationCidrBlock }}"
                    ports: [8000-10000]




        # This is to support boto v2 modules which do not yet support boto v3
        # e.g. community.aws.ec2_vpc_route_table_info
        - name: Check environment variables have been set
          fail: msg="Please set '{{ item }}' environment variable"
          when: not lookup('env', item) or lookup('env', item) == ""
          loop:
            - AWS_ACCESS_KEY_ID
            - AWS_SECRET_ACCESS_KEY
            - AWS_KEYPAIR_NAME

        - name: install python packages
          pip:
            name: "{{ item }}"
          loop:
            - boto
            - boto3
            - ansible

      tags:
        - always


    - block:
        - name: delete ASG
          community.aws.ec2_asg:
            name: "{{ asg_name }}"
            state: absent

        - name: delete ASG_LC
          community.aws.ec2_lc:
            name: "{{ asg_lc_name }}"
            state: absent

        - name: get vpc ID from VPC name
          amazon.aws.ec2_vpc_net_info:
            filters:
              "tag:Name": "{{ vpc_name }}"
          register: vpcs_match

        # delete Security Groups
        - name: get security groups
          amazon.aws.ec2_group_info:
            filters:
              vpc-id: "{{ item.vpc_id }}"
          loop: "{{ vpcs_match.vpcs }}"
          register: sg_match
        - name: delete security groups
          amazon.aws.ec2_group:
            group_id: "{{ item.group_id }}"
            state: absent
          loop: "{{ sg_match | community.general.json_query('results[*].security_groups[*]') | default([[]]) | first }}"
          when: not item.group_name == "default"

        # delete Route Tables
        - name: get vpc route tables
          community.aws.ec2_vpc_route_table_info:
            region: "{{ aws_region }}"
            filters:
              vpc_id: "{{ item.vpc_id }}"
          loop: "{{ vpcs_match.vpcs }}"
          register: vpc_route_tables
        - name: delete vpc route table
          community.aws.ec2_vpc_route_table:
            region: "{{ aws_region }}"
            route_table_id: "{{ item.id }}"
            lookup: id
            state: absent
          loop: "{{ vpc_route_tables | community.general.json_query('results[*].route_tables[*]') | default([[]]) | first }}"
          when: item.associations | selectattr('main') | length == 0

        # delete IGW
        - name: delete vpc IGW
          community.aws.ec2_vpc_igw:
            vpc_id: "{{ item.vpc_id }}"
            region: "{{ aws_region }}"
            state: absent
          loop: "{{ vpcs_match.vpcs }}"

        # delete NACLs
        - name: get vpc NACLs
          community.aws.ec2_vpc_nacl_info:
            region: "{{ aws_region }}"
            filters:
              'vpc-id': "{{ item.vpc_id }}"
          loop: "{{ vpcs_match.vpcs }}"
          register: vpc_nacls
        - name: delete vpc NACLs
          community.aws.ec2_vpc_nacl:
            nacl_id: "{{ item.nacl_id }}"
            state: absent
          loop: "{{ vpc_nacls | community.general.json_query('results[*].nacls[*]') | default([[]]) | first }}"
          when: not item.is_default

        # delete Subnets
        - name: get vpc subnets
          amazon.aws.ec2_vpc_subnet_info:
            filters:
              vpc-id: "{{ item.vpc_id }}"
          loop: "{{ vpcs_match.vpcs }}"
          register: vpc_subnets
        - name: delete vpc subnets
          amazon.aws.ec2_vpc_subnet:
            vpc_id: "{{ item.vpc_id }}"
            cidr: "{{ item.cidr_block }}"
            state: absent
          loop: "{{ vpc_subnets | community.general.json_query('results[*].subnets[*]') | default([[]]) | first }}"

        # delete VPC
        - name: delete vpc
          amazon.aws.ec2_vpc_net:
            name: "{{ item.tags.Name }}"
            cidr_block: ["{{ item.cidr_block }}"]
            purge_cidrs: true
            state: absent
          loop: "{{ vpcs_match.vpcs }}"

      tags:
        - delete
        - never


    - name: Get AMI ID
      amazon.aws.ec2_ami_info:
        owner: "{{ ami_image.owner }}"
        filters:
          name: "{{ ami_image.name }}*"
      register: ami_result

    - name: create VPC
      amazon.aws.ec2_vpc_net:
        region: "{{ aws_region }}"
        name: "{{ vpc_name }}"
        cidr_block: "{{ vpc_cidr_block }}"
      register: create_vpc
    - name: "set fact: VPC ID"
      set_fact:
        vpc_id: "{{ create_vpc.vpc.id }}"

    - name: create VPC subnets
      amazon.aws.ec2_vpc_subnet:
        vpc_id: "{{ vpc_id }}"
        region: "{{ aws_region }}"
        cidr: "{{ item.value.cidr }}"
        az: "{{ item.value.az }}"
        map_public: "{{ item.key is match('public') | ternary('yes', 'no', 'no') }}"
        tags:
          Name: "{{ item.key }}"
      with_dict: "{{ vpc_subnets }}"
      register: created_vpc_subnets

    - name: create VPC security groups
      amazon.aws.ec2_group:
        vpc_id: "{{ vpc_id }}"
        region: "{{ aws_region }}"
        name: "{{ item.name }}"
        description: "{{ item.description }}"
        rules: "{{ item.rules }}"
      loop: "{{ vpc_security_groups }}"
      register: created_vpc_security_groups

    - name: create ec2 VPC internet gateway
      community.aws.ec2_vpc_igw:
        vpc_id: "{{ vpc_id }}"
        region: "{{ aws_region }}"
      register: vpc_igw

    - name: create vpc nacl
      community.aws.ec2_vpc_nacl:
        vpc_id: "{{ vpc_id }}"
        region: "{{ aws_region }}"
        name: "{{ vpc_acl_name }}"
        subnets: "{{ created_vpc_subnets.results | map(attribute='subnet') | selectattr('map_public_ip_on_launch') | map(attribute='id') | list }}"
        ingress:  # incoming connections
          - [100, 'tcp', 'allow', "{{ destinationCidrBlock }}", null, null, 22, 22]  # tcp/ssh
          - [200, 'udp', 'allow', "{{ destinationCidrBlock }}", null, null, 8000, 10000]  # udp/sol
          - [300, 'tcp', 'allow', "{{ destinationCidrBlock }}", null, null, 1024, 65535]  # tcp/ephemeral
          # - [100, 'icmp', 'allow', "{{ destinationCidrBlock }}", 0, 8]  # icmp
          # - [200, 'tcp', 'allow', "{{ destinationCidrBlock }}", null, null, 80, 80]  # tcp/http
        egress:  # outgoing connections
          - [100, 'tcp', 'allow', "{{ destinationCidrBlock }}", null, null, 80, 80]  # tcp/http
          - [200, 'tcp', 'allow', "{{ destinationCidrBlock }}", null, null, 443, 443]  # tcp/https
          - [300, 'udp', 'allow', "{{ destinationCidrBlock }}", null, null, 8000, 10000]  # udp/sol
          - [400, 'tcp', 'allow', "{{ destinationCidrBlock }}", null, null, 1024, 65535]  # tcp/ephemeral
          # - [100, 'icmp', 'allow', "{{ destinationCidrBlock }}", 0, 8]  # icmp
          # - [200, 'tcp', 'allow', "{{ destinationCidrBlock }}", null, null, 22, 22]

    - name: create ec2 public route table
      community.aws.ec2_vpc_route_table:
        vpc_id: "{{ vpc_id }}"
        region: "{{ aws_region }}"
        subnets: "{{ created_vpc_subnets.results | map(attribute='subnet') | selectattr('map_public_ip_on_launch') | map(attribute='id') | list }}"
        routes:
          - dest: "{{ destinationCidrBlock }}"
            gateway_id: "{{ vpc_igw.gateway_id }}"

    - name: create ASG_LC
      community.aws.ec2_lc:
        vpc_id: "{{ vpc_id }}"
        aws_region: "{{ aws_region }}"
        name: "{{ asg_lc_name }}"
        image_id: "{{ ( ami_result.images | sort(attribute='creation_date') | last ).image_id }}"
        key_name: "{{ key_name }}"
        security_groups: "{{ created_vpc_security_groups.results | map(attribute='group_id') | list }}"
        instance_type: "{{ instance_type }}"
        assign_public_ip: true
        ebs_optimized: true
        instance_monitoring: true
        volumes:
          - device_name: /dev/sda1
            delete_on_termination: true
            volume_size: "{{ volume_size }}"
            volume_type: io1
            iops: volume_size | int * 50  # 50 iops per GB for io1 (50:1 ratio) - 1250
        user_data: |
          #!/bin/bash
          echo "end - $(date +%s) - $(date)" >userdata.log
      register: launch_config

    - name: create ASG
      community.aws.ec2_asg:
        name: "{{ asg_name }}"
        region: "{{ aws_region }}"
        availability_zones: "{{ created_vpc_subnets.results | map(attribute='subnet.availability_zone') | list | unique }}"
        vpc_zone_identifier: "{{ created_vpc_subnets.results | map(attribute='subnet') | selectattr('map_public_ip_on_launch') | map(attribute='id') | list }}"
        launch_config_name: "{{ launch_config.name }}"
        # mixed_instances_policy:
        #   instance_types:
        #     - t3a.2xlarge
        #     - t3.2xlarge
        #     - t2.2xlarge
        min_size: 0
        desired_capacity: 1
        max_size: 1
        replace_all_instances: true
        wait_for_instances: true
        tags:
          - environment: test
            propagate_at_launch: true
      register: autoscaling_group




- name: create ansible group
  connection: local
  gather_facts: false
  hosts: localhost
  vars:
    ansible_user: 'ubuntu' #fix this location
    default_user: 'ubuntu' #fix this location
  tags:
    - application
  tasks:

    - name: Gather information on ASG Instances
      community.aws.ec2_asg_info:
        name: "{{ asg_name }}"
      register: autoscaling_group_instances
    
    - name: Gather information on ec2 instances
      community.aws.ec2_instance_info:
        instance_ids: "{{  autoscaling_group_instances | community.general.json_query('results[*].instances[*].instance_id') | default([[]])| first }}"
      register: ec2_instances
    
    - name: create instance group
      add_host:
        name: "{{ item.instance_id }}"
        ansible_host: "{{ item.public_ip_address }}"
        ansible_user: "{{ default_user }}"
        ansible_ssh_extra_args: '-o StrictHostKeyChecking=no'
        group: asg_instances
      loop: "{{ ec2_instances.instances }}"
      loop_control:
        label: "{{ item.instance_id }}"

    - name: List ssh endpoints
      debug: msg="ssh {{ default_user }}@{{ item.public_ip_address | default("unknown") }}"
      loop: "{{ ec2_instances.instances }}"
      loop_control:
        label: "{{ item.instance_id }}"

- name: configure instances
  gather_facts: true
  hosts: asg_instances
  tags:
    - application
  vars:
    default_user: 'ubuntu'
    solana_dir: /opt/solana
    solana_environment_url: 'http://devnet.solana.com'
    solana_validator_key:
      name: 'test_key' # public: 'testCcDbxeTvA138gUVeYu5SdDYyjKCxWxmyuDbGJhQ'
      private: '[238,212,138,196,165,50,98,49,89,57,132,66,89,245,172,220,46,228,57,43,169,27,126,97,240,168,68,34,159,164,213,13,13,59,115,2,12,129,211,203,224,38,65,161,15,197,136,92,168,32,163,195,108,86,220,103,131,230,121,183,235,129,159,131]'
    sysctl_config:
      net.ipv4.ip_forward: 1
      net.core.rmem_default: 134217728
      net.core.rmem_max: 134217728
      net.core.wmem_default: 134217728
      net.core.wmem_max: 134217728
      vm.max_map_count: '500000'
      vm.swappiness: 0

  tasks:

    - ping:
  
    - name: root activities
      block:
        - name: Set timezone to Europe/London
          community.general.timezone:
            name: Europe/London
            
        - name: Update distro
          apt:
            update_cache: true
            upgrade: true

        - name: install packages
          apt:
            name: "{{ item }}"
          loop:
            - tuned
            - atop
            - htop
            - sysstat
            
        - shell: tuned-adm profile throughput-performance

        - name: set sysctl settings
          ansible.posix.sysctl:
            name: "{{ item.key }}"
            value: "{{ item.value }}"
            sysctl_set: yes
          with_dict: "{{ sysctl_config }}"

        - name: Add or modify hard nofile limits for wildcard domain
          community.general.pam_limits:
            domain: '*'
            limit_type: hard
            limit_item: nofile
            value: "{{ sysctl_config['vm.max_map_count'] }}"
            
        - name: Create logrotate file
          copy:
            dest: /etc/logrotate.d/sol
            mode: '0644'
            content: |
              {{ solana_dir }}/logs/solana-validator.log {
                rotate 7
                daily
                missingok
                postrotate
                  systemctl kill -s USR1 sol.service
                endscript
              }
        
        - name: Create sol.service
          copy:
            dest: /etc/systemd/system/sol.service
            mode: '0644'
            content: |
              [Unit]
              Description=Solana Validator
              After=network.target
              Wants=solana-sys-tuner.service
              StartLimitIntervalSec=0
              
              [Service]
              Type=simple
              Restart=always
              RestartSec=5
              User={{ default_user }}
              LimitNOFILE={{ sysctl_config['vm.max_map_count'] }}
              LogRateLimitIntervalSec=0
              Environment="PATH=/bin:/usr/bin:/home/{{ default_user }}/.local/share/solana/install/active_release/bin"
              ExecStart={{ solana_dir }}/bin/validator.sh
              
              [Install]
              WantedBy=multi-user.target

        - name: Create solana-sys-tuner.service
          copy:
            dest: /etc/systemd/system/solana-sys-tuner.service
            mode: '0644'
            content: |
              [Unit]
              Description=Solana Sys Tuner
              After=network.target
              StartLimitIntervalSec=0
              
              [Service]
              Type=simple
              Restart=always
              RestartSec=5
              User={{ default_user }}
              LimitNOFILE={{ sysctl_config['vm.max_map_count'] }}
              LogRateLimitIntervalSec=0
              Environment="PATH=/bin:/usr/bin:/home/{{ default_user }}/.local/share/solana/install/active_release/bin"
              ExecStart={{ solana_dir }}/bin/solana-sys-tuner.sh
              
              [Install]
              WantedBy=multi-user.target

        - name: Create custom solana-dev-setup
          copy:
            dest: /usr/local/bin/solana-dev-setup
            mode: '0775'
            content: |
              #!/bin/bash
              curl https://sh.rustup.rs -sSf | sh -s -- -y
              source ~/.cargo/env
              rustup component add rustfmt
              rustup update
              git clone --single-branch --branch master https://github.com/solana-labs/solana.git ~/solana && cd ~/solana
              cargo build --release &> ~/solana-build.log
              # ./run.sh &> ../solana-run.log &
    
    
        - name: Create custom solana-bench-dev
          copy:
            dest: /usr/local/bin/solana-bench-dev
            mode: '0775'
            content: |
              #!/bin/bash
              NDEBUG=1 ~/solana/multinode-demo/bench-tps.sh --entrypoint devnet.solana.com:8001 --faucet devnet.solana.com:9900 --duration 60 --tx_count 50

        - name: create solana directory
          file:
            path: "{{ solana_dir }}"
            mode: '0755'
            owner: "{{ ansible_user }}"
            group: "{{ ansible_user }}"
            state: directory


      become: true



    - name: create solana subdir
      file:
        path: "{{ solana_dir }}/{{ item }}"
        state: directory
        mode: '0755'
      loop:
        - bin
        - logs
        - keys

    - name: Create validator.sh
      copy:
        dest: "{{ solana_dir }}/bin/validator.sh"
        mode: '0740'
        content: |
          #!/bin/bash
          solana-validator \
            --identity     {{ solana_dir }}/keys/validator-keypair.json \
            --vote-account {{ solana_dir }}/keys/vote-account-keypair.json \
            --ledger       {{ solana_dir }}/validator-ledger \
            --rpc-port     8899 \
            --entrypoint   devnet.solana.com:8001 \
            --limit-ledger-size \
            --log          {{ solana_dir }}/logs/solana-validator.log
          EOF

    - name: Create solana-sys-tuner.sh
      copy:
        dest: "{{ solana_dir }}/bin/solana-sys-tuner.sh"
        mode: '0740'
        content: |
          #!/bin/bash
          sudo /home/{{ default_user }}/.local/share/solana/install/active_release/bin/solana-sys-tuner \
               --user {{ default_user }} \
               &> {{ solana_dir }}/logs/sys-tuner.log

    - name: get solana installer
      get_url:
        url: https://release.solana.com/stable/solana-install-init-x86_64-unknown-linux-gnu
        dest: "{{ solana_dir }}/bin/solana-install-init"
        mode: '0774'

    # - name: get solana release
    #   get_uri:
    #     url: https://github.com/solana-labs/solana/releases/download/v1.4.19/solana-release-x86_64-unknown-linux-gnu.tar.bz2
    #     dest: ~/solana-release-x86_64-unknown-linux-gnu.tar.bz2
    #     mode: '0664'

    - name: install solana client
      shell: 
        executable: /bin/bash
        chdir: "{{ solana_dir }}"
        cmd: |
          source ~/.profile
          
          which solana || ./bin/solana-install-init stable


    - name: record solana versions
      shell: 
        executable: /bin/bash
        chdir: "{{ solana_dir }}"
        cmd: |
          source ~/.profile
          
          solanapath=$(echo $PATH | cut -d: -f1)
          for file in $(find $solanapath -maxdepth 1 -type f); do $file -V; done &> logs/solana-versions.txt

    - name: install solana validator
      shell:
        executable: /bin/bash
        chdir: "{{ solana_dir }}"
        cmd: |
          source ~/.profile
          
          # Environment
          echo Set Environment:
          solana config set --url {{ solana_environment_url }}
          
          # Validator Keygen - unique per solana user
          echo -n '{{ solana_validator_key.private }}' > keys/validator-keypair.json
          echo Get Validator Keypair Public Key:
          solana-keygen pubkey keys/validator-keypair.json > keys/validator-keypair.pubkey
          cat keys/validator-keypair.pubkey
          echo Set Validator Keypair:
          solana config set --keypair keys/validator-keypair.json
          
          # Setup Balance
          echo Airdrop:
          solana airdrop 5
          echo Balance:
          solana balance
          echo Balance Lamports:
          solana balance --lamports
          
          # Vote Account Keygen
          echo Gen Vote Account KeyPair:
          [ ! -f 'keys/vote-account-keypair.json' ] && solana-keygen new -o keys/vote-account-keypair.json --no-passphrase
          echo Get Vote Account Keypair Public Key:
          solana-keygen pubkey keys/vote-account-keypair.json > keys/vote-account-keypair.pubkey
          cat keys/vote-account-keypair.pubkey
          echo Set VoteAccount:
          solana create-vote-account keys/vote-account-keypair.json keys/validator-keypair.json
          
          # Start vanity keygen
          echo Starting Vanity KeyGen:
          #solana-keygen grind --starts-with cwhee1:2 &> logs/cloudwheel-vanity.log &



    - shell:
        executable: /bin/bash
        chdir: "{{ solana_dir }}"
        cmd: |
          source ~/.profile
          
          # # Setup development env
          # apt -y install libssl-dev libudev-dev pkg-config zlib1g-dev llvm clang make && \
          # su -l {{ default_user }} -c "
          #   /usr/local/bin/solana-dev-setup
          # ">logs/dev-setup.log &


    - block:
    
        - name: Reload service logrotate
          systemd:
            name: logrotate.service
            enabled: true
            daemon_reload: true
            state: reloaded

        - name: Reload service solana-sys-tuner
          systemd:
            name: solana-sys-tuner.service
            enabled: true
            daemon_reload: true
            state: restarted
  
        - name: Reload service solana
          systemd:
            name: sol.service
            enabled: true
            daemon_reload: true
            state: started

      become: true

    - name: diagnostic links
      shell: 
        executable: /bin/bash
        cmd: |
          source ~/.profile
          
          echo "https://metrics.solana.com:3000/d/monitor-edge/cluster-telemetry-edge?refresh=60s&orgId=2&var-datasource=Solana%20Metrics%20(read-only)&var-testnet=devnet&var-hostid=$(cat {{ solana_dir }}/keys/validator-keypair.pubkey)" > links.txt

    - name: List ssh endpoints
      debug: msg="ssh {{ default_user }}@{{ ansible_host | default("unknown") }}"

    - name: retrieve pub validator-keypair
      slurp:
        src: "{{ solana_dir }}/keys/validator-keypair.pubkey"
      register: validator_key
    - name: retrieve pub vote-account-keypair
      slurp:
        src: "{{ solana_dir }}/keys/vote-account-keypair.pubkey"
      register: voter_key
      
    - set_fact:
        validator_pubkey: "{{ validator_key.content | b64decode | trim}}"
        voter_pubkey: "{{ voter_key.content | b64decode | trim }}"


    - name: retrieve list of cluster nodes
      uri:
        url: http://devnet.solana.com
        method: POST
        return_content: yes
        status_code: 200
        body: '{"jsonrpc":"2.0","id":1, "method":"getClusterNodes"}'
        body_format: json
      register: result
      until: result.json.result | length > 0
      retries: 5
      delay: 2
    
    - name: debug - validators
      debug: msg="{{ item }}"
      loop: "{{ result.json | community.general.json_query('result[*].pubkey') }}"



    - name: retrieve list of vote accounts
      uri:
        url: http://devnet.solana.com
        method: POST
        return_content: yes
        status_code: 200
        body: '{"jsonrpc":"2.0","id":1, "method":"getVoteAccounts"}'
        body_format: json
      register: result
      until: result.json.result | length > 0
      retries: 5
      delay: 2
    
    - name: debug
      debug: msg="{{ result.json.result | to_nice_yaml }}"
      #debug: msg="{{ result.json.result | community.general.json_query('result[*].pubkey') | default([[]])| first }}"

    - debug: msg="{{ validator_pubkey }} - {{ voter_pubkey }}"
